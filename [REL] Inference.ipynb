{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afe341bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:42:58.702433Z",
     "start_time": "2022-02-02T17:42:06.582791Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Reshape, Concatenate, Activation\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from transformers import TFAutoModel\n",
    "from tensorflow.keras import backend as K\n",
    "from focal_loss import sparse_categorical_focal_loss\n",
    "from transformers import AutoModel\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras_contrib.layers import CRF\n",
    "# import tensorflow_hub as hub\n",
    "# import tensorflow_text as text\n",
    "import pythainlp\n",
    "import spacy_thai\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "from string import punctuation\n",
    "import pickle\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d33a2e24",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:42:59.802766Z",
     "start_time": "2022-02-02T17:42:59.672323Z"
    }
   },
   "outputs": [],
   "source": [
    "def load_model(filename, customize = True):\n",
    "    \n",
    "    model = tf.keras.models.load_model(filename, compile = not customize)\n",
    "    \n",
    "    return model\n",
    "\n",
    "def load_mapping(filename):\n",
    "    \n",
    "    with open(filename, 'rb') as dict_:\n",
    "        \n",
    "        mapping = pickle.load(dict_)\n",
    "        \n",
    "    return mapping\n",
    "\n",
    "def check_condition(condition):\n",
    "\n",
    "        if condition:\n",
    "            return 'True'\n",
    "        else:\n",
    "            return 'False'\n",
    "        \n",
    "def preprocess(text, tokenize_method = 'syllable'):\n",
    "    \n",
    "    list_tokenizer = {'word' : pythainlp.word_tokenize,\n",
    "                 'subword' : pythainlp.subword_tokenize,\n",
    "                 'syllable' : pythainlp.syllable_tokenize}\n",
    "    \n",
    "    tokenizer = list_tokenizer[tokenize_method]\n",
    "    \n",
    "    list_text = tokenizer(text)\n",
    "    \n",
    "    NER_mapping = {\n",
    "                   'tok2idx' : load_mapping('mapping/NER/tok2idx.pickle'),\n",
    "                   'tag2idx' : load_mapping('mapping/NER/tag2idx.pickle'),\n",
    "                   'pos2idx' : load_mapping('mapping/NER/pos2idx.pickle'),\n",
    "                   'max_len' : load_mapping('mapping/NER/max_len.pickle')\n",
    "                  }\n",
    "    \n",
    "    NER_mapping['digit2idx'] = {'True' : 1, 'False' : 0, '<PAD>' : 2}\n",
    "    NER_mapping['punc2idx'] = {'True' : 1, 'False' : 0, '<PAD>' : 2}\n",
    "    NER_mapping['vowel2idx'] = {'True' : 1, 'False' : 0, '<PAD>' : 2}\n",
    "    \n",
    "\n",
    "    thai_vowel = 'ะาิีุุึืโเแัำไใฤๅฦ'\n",
    "    contain_digit_condition = [check_condition(any(char.isdigit() for char in text)) for text in list_text]\n",
    "    contain_punc_condition = [check_condition(any(char in punctuation for char in text)) for text in list_text]\n",
    "    contain_vowel_condition = [check_condition(any(char in thai_vowel for char in text)) for text in list_text]\n",
    "    \n",
    "    words_input = []\n",
    "    pos_input = []\n",
    "    \n",
    "    for word in list_text:\n",
    "        try:\n",
    "            words_input.append(NER_mapping['tok2idx'][word])\n",
    "        except:\n",
    "            words_input.append(NER_mapping['tok2idx']['<UNK>'])\n",
    "            \n",
    "    for word in list_text:\n",
    "        try:\n",
    "            pos_input.append(NER_mapping['pos2idx'][word])\n",
    "        except:\n",
    "            pos_input.append(NER_mapping['pos2idx']['<UNK>'])\n",
    "            \n",
    "    contain_digit_input = [NER_mapping['digit2idx'][i] for i in contain_digit_condition]\n",
    "    contain_punc_input  = [NER_mapping['punc2idx'][i] for i in contain_punc_condition]\n",
    "    contain_vowel_input = [NER_mapping['vowel2idx'][i] for i in contain_vowel_condition]\n",
    "    \n",
    "    NER_input = {'list_text' : list_text,\n",
    "                 'words_idx' : words_input,\n",
    "                 'pos_idx' : pos_input,\n",
    "                 'contain_digit_idx' : contain_digit_input,\n",
    "                 'contain_punc_idx' : contain_punc_input,\n",
    "                 'contain_vowel_idx' : contain_vowel_input}\n",
    "    \n",
    "    return NER_input, NER_mapping\n",
    "\n",
    "def return_span(entities):\n",
    "    \n",
    "\n",
    "    entities.append(('.', 'O'))\n",
    "    text = ''.join([i[0] for i in entities])\n",
    "    seek = 0\n",
    "    start_seek = None\n",
    "    start_entity_type = None\n",
    "    spans = []\n",
    "\n",
    "    for word, named_entity in entities:\n",
    "\n",
    "        if len(named_entity.split('-')) == 1:\n",
    "            entity_prefix = named_entity[0] # O\n",
    "\n",
    "        else: \n",
    "            entity_prefix, entity_type = named_entity.split('-') # B , LOCATION\n",
    "            \n",
    "        \n",
    "        \n",
    "        if entity_prefix == 'B':\n",
    "            if start_seek is None:\n",
    "                start_seek = seek # --> assign first span at B tag\n",
    "                start_entity_type = entity_type\n",
    "                \n",
    "            elif start_seek is not None:\n",
    "                \n",
    "                end_seek = seek\n",
    "                spans.append((start_seek, end_seek, entity_type, text[start_seek:end_seek]))\n",
    "                start_seek = seek\n",
    "                start_entity_type = entity_type\n",
    "\n",
    "        elif entity_prefix == 'I':\n",
    "            if start_seek is None:\n",
    "            \n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                if entity_type == start_entity_type:\n",
    "\n",
    "                    pass\n",
    "                \n",
    "                else:\n",
    "                    end_seek = seek\n",
    "                    spans.append((start_seek, end_seek, entity_type, text[start_seek:end_seek]))\n",
    "                    start_seek = None\n",
    "                    start_entity_type = None\n",
    "\n",
    "        \n",
    "        \n",
    "        else:\n",
    "            if start_seek is not None: # ถ้าไม่มี start seek \n",
    "                end_seek = seek\n",
    "                spans.append((start_seek, end_seek, entity_type, text[start_seek:end_seek]))\n",
    "                start_seek = None\n",
    "                entity_type = None\n",
    "       \n",
    "        seek += len(word)\n",
    "    \n",
    "    return spans\n",
    "\n",
    "def run_NER(raw_text):\n",
    "    \n",
    "    NER_input, NER_mapping = preprocess(raw_text)\n",
    "    model = load_model('model/NER/NER_model_v2_26_1_2022.h5')\n",
    "    max_len = NER_mapping['max_len']\n",
    "    \n",
    "    NER_input['padded_words_idx'] = list(pad_sequences([NER_input['words_idx']], maxlen = max_len, \n",
    "                                                       padding = 'post', value = NER_mapping['tok2idx']['<PAD>']))\n",
    "    NER_input['padded_pos_idx'] = list(pad_sequences([NER_input['pos_idx']], maxlen = max_len, \n",
    "                                                     padding = 'post', value = NER_mapping['pos2idx']['<PAD>']))\n",
    "    NER_input['padded_contain_digit_idx'] = list(pad_sequences([NER_input['contain_digit_idx']], maxlen = max_len, \n",
    "                                                               padding = 'post', value = NER_mapping['digit2idx']['<PAD>']))\n",
    "    NER_input['padded_contain_punc_idx'] = list(pad_sequences([NER_input['contain_punc_idx']], maxlen = max_len, \n",
    "                                                              padding = 'post', value = NER_mapping['punc2idx']['<PAD>']))\n",
    "    NER_input['padded_contain_vowel_idx'] = list(pad_sequences([NER_input['contain_vowel_idx']], maxlen = max_len, \n",
    "                                                               padding = 'post', value = NER_mapping['vowel2idx']['<PAD>']))\n",
    "   \n",
    "    \n",
    "    \n",
    "    X_test = [np.array(NER_input['padded_words_idx']), \n",
    "              np.array(NER_input['padded_pos_idx']),\n",
    "              np.array(NER_input['padded_contain_digit_idx']),\n",
    "              np.array(NER_input['padded_contain_punc_idx']),\n",
    "              np.array(NER_input['padded_contain_vowel_idx'])]\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis = 2)[0]\n",
    "    \n",
    "    idx2tag = dict([(v, k) for k,v in NER_mapping['tag2idx'].items()])\n",
    "    prediction = [idx2tag[i] for i in y_pred]\n",
    "    \n",
    "    entities = [*zip(NER_input['list_text'], prediction)]\n",
    "    \n",
    "    spans = return_span(entities)\n",
    "\n",
    "    return raw_text, spans\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4265b7b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:43:08.542901Z",
     "start_time": "2022-02-02T17:42:59.853976Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocessed_span_for_REL' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-bcb438dfc43a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun_NER\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'พบศพนายสมชาย และพบมีดสั้นตกอยู่ข้างๆ คาดว่าน่าจะเป็นฝีมือของนายสมปอง ที่เป็นเจ้าของรถยนต์โตโยต้า'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mpreprocessed_span_for_REL\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'preprocessed_span_for_REL' is not defined"
     ]
    }
   ],
   "source": [
    "t, s = run_NER('พบศพนายสมชาย และพบมีดสั้นตกอยู่ข้างๆ คาดว่าน่าจะเป็นฝีมือของนายสมปอง ที่เป็นเจ้าของรถยนต์โตโยต้า')\n",
    "preprocessed_span_for_REL(t, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0b781d9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:43:13.950181Z",
     "start_time": "2022-02-02T17:43:13.927785Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocessed_span_for_REL(raw_text, spans):\n",
    "    \n",
    "    possible_pairs = list(itertools.combinations(spans, 2))\n",
    "    reverse_pairs = [(i[1], i[0]) for i in possible_pairs]\n",
    "    possible_pairs += reverse_pairs\n",
    "    \n",
    "    condition = [\n",
    "                    ('PERSON', 'VEHICLE'), ('VEHICLE', 'LP'), ('PERSON', 'PERSON'), ('PERSON', 'WEAPON'),\n",
    "                    ('PERSON', 'LOCATION'), ('PERSON', 'DATE'), ('PERSON', 'TIME'), ('VEHICLE', 'COLOR'),\n",
    "                    ('OBJECT', 'LOCATION'), ('PERSON', 'ORGANIZATION')\n",
    "                ]\n",
    "    \n",
    "    possible_pairs = [i for i in possible_pairs if (i[0][2], i[1][2]) in condition]\n",
    "    \n",
    "    pairs_df = pd.DataFrame()\n",
    "    \n",
    "    tokenizer = pythainlp.word_tokenize\n",
    "    \n",
    "    for tok_1, tok_2 in possible_pairs:\n",
    "        \n",
    "        \n",
    "        E1 = tok_1[3]\n",
    "        E2 = tok_2[3]\n",
    "        E1_entity = tok_1[2]\n",
    "        E2_entity = tok_2[2]\n",
    "        \n",
    "        start_E1, start_E2, end_E1, end_E2 = tok_1[0], tok_2[0], tok_1[1], tok_2[1]\n",
    "        \n",
    "        before_E1 = raw_text[:start_E1]\n",
    "        before_E2 = raw_text[:start_E2]\n",
    "        after_E1 = raw_text[end_E1:]\n",
    "        after_E2 = raw_text[end_E2:]\n",
    "        \n",
    "        btw_E1_E2 = raw_text[end_E1:start_E2]\n",
    "        \n",
    "        if end_E1 > start_E2:\n",
    "            btw_E1_E2 = raw_text[start_E2:end_E1]\n",
    "            \n",
    "        tmp = pd.DataFrame({'text' : [raw_text],\n",
    "                            'E1' : [tokenizer(E1)],\n",
    "                            'E2' : [tokenizer(E2)],\n",
    "                            'E1_entity' : [E1_entity],\n",
    "                            'E2_entity' : [E2_entity],\n",
    "                            'before_E1' : [tokenizer(before_E1)],\n",
    "                            'before_E2' : [tokenizer(before_E2)],\n",
    "                            'after_E1' : [tokenizer(after_E1)],\n",
    "                            'after_E2' : [tokenizer(after_E2)],\n",
    "                            'between_E1_E2' : [tokenizer(btw_E1_E2)]})\n",
    "        \n",
    "        pairs_df = pd.concat([pairs_df, tmp], ignore_index = True)\n",
    "            \n",
    "        \n",
    "        \n",
    "    return pairs_df\n",
    "\n",
    "def convert_to_idx(pairs_df, mapping):\n",
    "    \n",
    "    columns = ['E1', 'E2', 'before_E1', 'before_E2', 'after_E1', 'after_E2', 'between_E1_E2']\n",
    "    \n",
    "    for col in columns:\n",
    "        \n",
    "        tt = []\n",
    "        \n",
    "        for i in pairs_df[col]:\n",
    "            t = []\n",
    "            for x in i:\n",
    "                try:\n",
    "                    t.append(mapping['tok2idx'][x])\n",
    "                except:\n",
    "                    t.append(mapping['tok2idx']['<UNK>'])\n",
    "            tt.append(t)\n",
    "            \n",
    "        pairs_df[col + '_idx'] = list(pad_sequences(tt, maxlen = mapping['max_len'], \n",
    "                                                    padding = 'post', value = mapping['tok2idx']['<PAD>']))\n",
    "            \n",
    "        \n",
    "    pairs_df['E1_entity_idx'] = pairs_df['E1_entity'].apply(lambda x: mapping['tag2idx'][x])\n",
    "    pairs_df['E2_entity_idx'] = pairs_df['E2_entity'].apply(lambda x: mapping['tag2idx'][x])\n",
    "    \n",
    "    \n",
    "    return pairs_df\n",
    "\n",
    "def run_REL(text):\n",
    "    \n",
    "    try:\n",
    "        \n",
    "        text, spans = run_NER(text)\n",
    "        REL_mapping = {\n",
    "                       'tok2idx' : load_mapping('mapping/REL/tok2idx.pickle'),\n",
    "                       'tag2idx' : load_mapping('mapping/REL/tag2idx.pickle'),\n",
    "                       'rel2idx' : load_mapping('mapping/REL/rel2idx.pickle'),\n",
    "                       'max_len' : load_mapping('mapping/REL/max_len.pickle')\n",
    "                      }\n",
    "        \n",
    "        pairs_df = preprocessed_span_for_REL(text, spans)\n",
    "        pairs_df = convert_to_idx(pairs_df, REL_mapping)\n",
    "\n",
    "        model = load_model('model/REL/REL_model_v2_27_1_2022.h5')\n",
    "\n",
    "        X_test = []\n",
    "        columns = ['E1_idx', 'E2_idx', 'E1_entity_idx', 'E2_entity_idx', \n",
    "                   'before_E1_idx', 'before_E2_idx', 'after_E1_idx', 'after_E2_idx', 'between_E1_E2_idx']\n",
    "\n",
    "        for col in columns:\n",
    "            col_list = []\n",
    "            for i in pairs_df[col]:\n",
    "                col_list.append(i)\n",
    "            X_test.append(np.array(col_list))\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = np.argmax(y_pred, axis = 1)\n",
    "        idx2rel = dict([(v, k) for k, v in REL_mapping['rel2idx'].items()])\n",
    "        prediction = [idx2rel[i] for i in y_pred]\n",
    "\n",
    "        pairs_df['prediction'] = prediction\n",
    "        pairs_df['E1'] = pairs_df['E1'].apply(lambda x: ''.join(x))\n",
    "        pairs_df['E2'] = pairs_df['E2'].apply(lambda x: ''.join(x))\n",
    "#         pairs_df = pairs_df[pairs_df['prediction'] != 'no_relation']\n",
    "        \n",
    "        \n",
    "        \n",
    "        ENTITIES_RESULT = {\n",
    "                            'success' : True, \n",
    "                            'input' : text,\n",
    "                            'keysEntities' : [\n",
    "                                {\n",
    "                                'type' : ent[2],\n",
    "                                'name' : ent[3],\n",
    "                                'mentionOffsets' : {'start' : ent[0], 'end' : ent[1], 'entity_index' : index}\n",
    "                                }  for index, ent in enumerate(spans)]\n",
    "                          }\n",
    "        \n",
    "        RELATION_RESULT = {\n",
    "                            'success' : True,\n",
    "                            'input' : text,\n",
    "                            'keyEntities' : ENTITIES_RESULT['keysEntities'],\n",
    "                            'relationships' : [\n",
    "                                                {\n",
    "                                                    'source' : row['E1'],\n",
    "                                                    'target' : row['E2'],\n",
    "                                                    'source_type' : row['E1_entity'],\n",
    "                                                    'target_type' : row['E2_entity'],\n",
    "                                                    'relation' : row['prediction']\n",
    "                                                } for index, row in pairs_df.iterrows()]\n",
    "        \n",
    "                          }\n",
    "        \n",
    "        \n",
    "    except:\n",
    "        \n",
    "        ENTITIES_RESULT = {'success' : False}\n",
    "        RELATION_RESULT = {'success' : False}\n",
    "        \n",
    "  \n",
    "    return ENTITIES_RESULT, RELATION_RESULT\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a3a7675",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:43:18.050948Z",
     "start_time": "2022-02-02T17:43:14.760792Z"
    }
   },
   "outputs": [],
   "source": [
    "entities, relation = run_REL('เมื่อเวลา 18.30 น. วันที่ 26 ม.ค. 65 พบศพนายมงคล ชัยมงคล ถูกแทงด้วยมีดสั้นที่กลางอกจนเสียชีวิต ทราบภายหลังว่าผู้ก่อเหตุคือ นายสมหวัง แซ่ตั้ง อายุ 25 ปี')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9d94e3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:43:18.098255Z",
     "start_time": "2022-02-02T17:43:18.084245Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'input': 'เมื่อเวลา 18.30 น. วันที่ 26 ม.ค. 65 พบศพนายมงคล ชัยมงคล ถูกแทงด้วยมีดสั้นที่กลางอกจนเสียชีวิต ทราบภายหลังว่าผู้ก่อเหตุคือ นายสมหวัง แซ่ตั้ง อายุ 25 ปี',\n",
       " 'keysEntities': [{'type': 'TIME',\n",
       "   'name': '18.30 น.',\n",
       "   'mentionOffsets': {'start': 10, 'end': 18, 'entity_index': 0}},\n",
       "  {'type': 'DATE',\n",
       "   'name': '26 ม.ค. 65',\n",
       "   'mentionOffsets': {'start': 26, 'end': 36, 'entity_index': 1}},\n",
       "  {'type': 'PERSON',\n",
       "   'name': 'นายมงคล ชัยมงคล ',\n",
       "   'mentionOffsets': {'start': 41, 'end': 57, 'entity_index': 2}},\n",
       "  {'type': 'WEAPON',\n",
       "   'name': 'มีดสั้น',\n",
       "   'mentionOffsets': {'start': 67, 'end': 74, 'entity_index': 3}},\n",
       "  {'type': 'PERSON',\n",
       "   'name': 'นายสมหวัง แซ่ตั้ง ',\n",
       "   'mentionOffsets': {'start': 123, 'end': 141, 'entity_index': 4}},\n",
       "  {'type': 'TIME',\n",
       "   'name': '25 ปี',\n",
       "   'mentionOffsets': {'start': 146, 'end': 151, 'entity_index': 5}}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "250455c3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-02-02T17:43:18.144606Z",
     "start_time": "2022-02-02T17:43:18.129945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'success': True,\n",
       " 'input': 'เมื่อเวลา 18.30 น. วันที่ 26 ม.ค. 65 พบศพนายมงคล ชัยมงคล ถูกแทงด้วยมีดสั้นที่กลางอกจนเสียชีวิต ทราบภายหลังว่าผู้ก่อเหตุคือ นายสมหวัง แซ่ตั้ง อายุ 25 ปี',\n",
       " 'keyEntities': [{'type': 'TIME',\n",
       "   'name': '18.30 น.',\n",
       "   'mentionOffsets': {'start': 10, 'end': 18, 'entity_index': 0}},\n",
       "  {'type': 'DATE',\n",
       "   'name': '26 ม.ค. 65',\n",
       "   'mentionOffsets': {'start': 26, 'end': 36, 'entity_index': 1}},\n",
       "  {'type': 'PERSON',\n",
       "   'name': 'นายมงคล ชัยมงคล ',\n",
       "   'mentionOffsets': {'start': 41, 'end': 57, 'entity_index': 2}},\n",
       "  {'type': 'WEAPON',\n",
       "   'name': 'มีดสั้น',\n",
       "   'mentionOffsets': {'start': 67, 'end': 74, 'entity_index': 3}},\n",
       "  {'type': 'PERSON',\n",
       "   'name': 'นายสมหวัง แซ่ตั้ง ',\n",
       "   'mentionOffsets': {'start': 123, 'end': 141, 'entity_index': 4}},\n",
       "  {'type': 'TIME',\n",
       "   'name': '25 ปี',\n",
       "   'mentionOffsets': {'start': 146, 'end': 151, 'entity_index': 5}}],\n",
       " 'relationships': [{'source': 'นายมงคล ชัยมงคล ',\n",
       "   'target': 'มีดสั้น',\n",
       "   'source_type': 'PERSON',\n",
       "   'target_type': 'WEAPON',\n",
       "   'relation': 'own_weapon'},\n",
       "  {'source': 'นายมงคล ชัยมงคล ',\n",
       "   'target': 'นายสมหวัง แซ่ตั้ง ',\n",
       "   'source_type': 'PERSON',\n",
       "   'target_type': 'PERSON',\n",
       "   'relation': 'crime_relation'},\n",
       "  {'source': 'นายมงคล ชัยมงคล ',\n",
       "   'target': '25 ปี',\n",
       "   'source_type': 'PERSON',\n",
       "   'target_type': 'TIME',\n",
       "   'relation': 'no_relation'},\n",
       "  {'source': 'นายสมหวัง แซ่ตั้ง ',\n",
       "   'target': '25 ปี',\n",
       "   'source_type': 'PERSON',\n",
       "   'target_type': 'TIME',\n",
       "   'relation': 'no_relation'},\n",
       "  {'source': 'นายมงคล ชัยมงคล ',\n",
       "   'target': '18.30 น.',\n",
       "   'source_type': 'PERSON',\n",
       "   'target_type': 'TIME',\n",
       "   'relation': 'crime_time_at'},\n",
       "  {'source': 'นายสมหวัง แซ่ตั้ง ',\n",
       "   'target': '18.30 น.',\n",
       "   'source_type': 'PERSON',\n",
       "   'target_type': 'TIME',\n",
       "   'relation': 'crime_time_at'},\n",
       "  {'source': 'นายมงคล ชัยมงคล ',\n",
       "   'target': '26 ม.ค. 65',\n",
       "   'source_type': 'PERSON',\n",
       "   'target_type': 'DATE',\n",
       "   'relation': 'crime_date_at'},\n",
       "  {'source': 'นายสมหวัง แซ่ตั้ง ',\n",
       "   'target': '26 ม.ค. 65',\n",
       "   'source_type': 'PERSON',\n",
       "   'target_type': 'DATE',\n",
       "   'relation': 'crime_date_at'},\n",
       "  {'source': 'นายสมหวัง แซ่ตั้ง ',\n",
       "   'target': 'นายมงคล ชัยมงคล ',\n",
       "   'source_type': 'PERSON',\n",
       "   'target_type': 'PERSON',\n",
       "   'relation': 'crime_relation'},\n",
       "  {'source': 'นายสมหวัง แซ่ตั้ง ',\n",
       "   'target': 'มีดสั้น',\n",
       "   'source_type': 'PERSON',\n",
       "   'target_type': 'WEAPON',\n",
       "   'relation': 'own_weapon'}]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6a3183",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
